
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "# CVXOPT runs extremely slowly for datasets in the 1000's of instances. As the Professor suggests in the homework, Libsvm is\n",
    "# highly performant. In fact libsvm is used under the hood in sklearn and hence the use of this library instead.\n",
    "\n",
    "# Follow instructions here to get cvxopt working. --- painful!!! \n",
    "#https://stackoverflow.com/questions/46009925/how-to-install-cvxopt-on-on-windows-10-on-python-3-6\n",
    "#from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and testing data in pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    column_names = ['digit', 'intensity', 'symmetry']\n",
    "    sep = '\\s+'\n",
    "    features_train = pd.read_table('http://www.amlbook.com/data/zip/features.train', sep=sep, names=column_names)\n",
    "    features_test = pd.read_table('http://www.amlbook.com/data/zip/features.test', sep=sep, names=column_names)\n",
    "    return features_train, features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set output labels for one-vs-all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_one_vs_all = dict(\n",
    "    {'zero_vs_all': 0,\n",
    "     'one_vs_all': 1,\n",
    "     'two_vs_all': 2,\n",
    "     'three_vs_all': 3,\n",
    "     'four_vs_all': 4,\n",
    "     'five_vs_all': 5,\n",
    "     'six_vs_all': 6,\n",
    "     'seven_vs_all': 7,\n",
    "     'eight_vs_all': 8,\n",
    "     'nine_vs_all': 9\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one-versus-all dataframe by adding outputs for the different classifiers to the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_vs_all_dataframe(df, classifiers):\n",
    "    # add binary labels to dataframe\n",
    "    one_vs_all = pd.DataFrame(df, copy=True)\n",
    "    for class_label, digit in classifiers.items():\n",
    "        labels = one_vs_all.loc[one_vs_all['digit'] == digit, 'digit']\n",
    "        labels.loc[:] = 1.0\n",
    "        one_vs_all[class_label] = labels\n",
    "        \n",
    "    one_vs_all.fillna(-1.0, inplace=True)\n",
    "    return one_vs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_one_vs_all = create_one_vs_all_dataframe(features_train, classifiers_one_vs_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset (inputs / outputs) for a specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df, class_label):\n",
    "    inputs = np.array(df.loc[:, ['intensity', 'symmetry']])\n",
    "    outputs = np.array(df.loc[:, class_label])\n",
    "    data = np.column_stack((inputs, outputs))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Support Vector Machine (SVM) class - this is not used due to how slow quadratic programming package is - left here for illustration of the general structure of algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\" Support Vector Machine (SVM) class. \"\"\"\n",
    "    \n",
    "    def __init__(self, Q=2, rbf=False):\n",
    "        \"\"\" Create a Support Vector Machine Algorithm (SVM). \n",
    "        Args:\n",
    "        Q (int): Degree of the polynomial kernel (Q >= 0)\n",
    "        rbf (bool): Use the RBF kernel instead of the polynomial kernel\n",
    "        \"\"\"\n",
    "        self.Q = Q\n",
    "        self.rbf = rbf\n",
    "        \n",
    "    def train(self, inputs, outputs, C=0.01, margin='hard'):    \n",
    "        \"\"\"Training using Support Vector Machine Algorithm (SVM).\n",
    "        Args:\n",
    "        inputs (np.ndarray): Input points.\n",
    "        outputs (np.ndarray): Targets.\n",
    "        C (float): Upper bound of Lagrange multiplier (C > 0.0)\n",
    "        margin (string): Margin to apply, 'hard' or 'soft'\n",
    "        Returns:\n",
    "        Tuple(np.ndarray, int): Weights, number of support vectors\n",
    "        \"\"\"\n",
    "        if margin not in ['hard','soft']:\n",
    "            raise ValueError('Margin must be \"hard\" or \"soft\".')\n",
    "        \n",
    "        xn = inputs\n",
    "        yn = outputs\n",
    "        N = len(xn)\n",
    "        \n",
    "        mat = []\n",
    "        for row_idx in range(0, N):\n",
    "            for col_idx in range(0, N):\n",
    "                if self.rbf:\n",
    "                    kernel = self.kernel_rbf(xn[row_idx], xn[col_idx])\n",
    "                else:\n",
    "                    kernel = self.kernel_poly(xn[row_idx], xn[col_idx])\n",
    "                val = yn[row_idx] * yn[col_idx] * kernel\n",
    "                mat.append(val)\n",
    "        mat = np.array(mat).reshape((N, N))\n",
    "        \n",
    "        # form matrices for quadratic programming solver\n",
    "        dim = len(xn[0])\n",
    "        P = matrix(mat, tc='d')\n",
    "        q = matrix(-np.ones(N), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "        A = matrix(yn, tc='d')\n",
    "        A = A.trans()\n",
    "        G = matrix(-np.identity(N), tc='d')\n",
    "        if margin == 'hard':\n",
    "            G = matrix(-np.identity(N), tc='d')\n",
    "            h = matrix(np.zeros(N), tc='d')\n",
    "        elif margin == 'soft':\n",
    "            G_zero = -np.identity(N)\n",
    "            h_zero = np.zeros(N)\n",
    "            G_C = np.identity(N)\n",
    "            h_C = C * np.ones(N)\n",
    "              \n",
    "            G = matrix(np.concatenate((G_zero, G_C)), tc='d')\n",
    "            h = matrix(np.concatenate((h_zero, h_C)), tc='d')\n",
    "        \n",
    "        #print('P = ', P)\n",
    "        #print('q = ', q)\n",
    "        #print('G = ', G)\n",
    "        #print('h = ', h)\n",
    "        #print('A = ', A)\n",
    "        #print('b = ', b)\n",
    "                \n",
    "        # call qp solver to compute weights\n",
    "        solvers.options['show_progress'] = False # supress solver output\n",
    "        \n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        alpha = np.array(list(sol['x']))\n",
    "        #print('alpha = ', sol['x'])\n",
    "        \n",
    "        weights = np.zeros(dim)\n",
    "        #sv_idxs = []\n",
    "        sv = []\n",
    "        sv_alphas = []\n",
    "        sv_outputs = []\n",
    "        for n in range(0, N):\n",
    "            if margin == 'hard':\n",
    "                weights += alpha[n] * yn[n] * xn[n]\n",
    "                if alpha[n] > 1e-5: # => xn[n] is support vector\n",
    "                    sv.append(xn[n])\n",
    "            elif margin == 'soft':\n",
    "                #print('alpha[{0}] = {1}'.format(n, alpha[n]))\n",
    "                if alpha[n] > 1e-5 and alpha[n] <= C: # => xn[n] is support vector\n",
    "                    #sv_idxs.append(n)\n",
    "                    sv.append(xn[n])\n",
    "                    sv_alphas.append(alpha[n])\n",
    "                    sv_outputs.append(yn[n])\n",
    "                    \n",
    "        # compute number of support vectors\n",
    "        num_sv = len(sv)\n",
    "        \n",
    "        if (num_sv == 0):\n",
    "            raise Exception('There are no support vectors.')\n",
    "        \n",
    "        bs = []\n",
    "        for m in range(0, num_sv):\n",
    "            if margin == 'hard':\n",
    "                b = (1. / sv_outputs[m]) - np.dot(weights.T, sv[m])\n",
    "            elif margin == 'soft':\n",
    "                b = sv_outputs[m]\n",
    "                for n in range(0, num_sv):\n",
    "                    if self.rbf:\n",
    "                        kernel = self.kernel_rbf(sv[n], sv[m])\n",
    "                    else:\n",
    "                        kernel = self.kernel_poly(sv[n], sv[m])\n",
    "                    b -= sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            bs.append(b)\n",
    "        bs_round = np.round(bs, 1)\n",
    "        #print('bs sv = ', bs)\n",
    "        \n",
    "        #print('bs = ', bs)\n",
    "        #print('bs_round = ', bs_round)\n",
    "        #print('np.unique(bs_round) = ', np.unique(bs_round))\n",
    "        \n",
    "        #if (len(np.unique(bs_round)) != 1):\n",
    "        #    raise Exception('All support vectors must produce the same value of b.')\n",
    "            \n",
    "        weights = np.insert(weights, 0, b)\n",
    "        #print('weights = ', weights)\n",
    "\n",
    "        if margin == 'hard':\n",
    "            return weights, num_sv\n",
    "        elif margin == 'soft':\n",
    "            return np.array(sv_alphas), np.array(sv), np.array(sv_outputs), b\n",
    "    \n",
    "    def binary_error(self, sv_alphas, sv, sv_outputs, b, inputs, outputs):\n",
    "        \"\"\" Evaluate binary classification error. \n",
    "        Args:\n",
    "        sv_alphas (np.ndarray): Support vector Lagrange multipliers.\n",
    "        sv (np.ndarray): Support vectors.\n",
    "        sv_outputs (np.ndarray): Support vector outputs.\n",
    "        b (float): Constant.\n",
    "        inputs (np.ndarray): Inputs.\n",
    "        outputs (np.ndarray): Outputs.\n",
    "        Returns (float): Binary classification error percentage\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        y = outputs\n",
    "        num_sv = len(sv)\n",
    "        \n",
    "        gs = []\n",
    "        for xm in x:\n",
    "            signal = 0.0\n",
    "            for n in range(0, num_sv):\n",
    "                if self.rbf:\n",
    "                    kernel = self.kernel_rbf(sv[n], xm)\n",
    "                else:\n",
    "                    kernel = self.kernel_poly(sv[n], xm)\n",
    "                signal += sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            signal += b\n",
    "            gs.append(signal)\n",
    "                \n",
    "        g = np.array(np.sign(gs))\n",
    "        return 100. * np.sum(y != g) / len(y)     \n",
    "    \n",
    "    # privates\n",
    "    def kernel_poly(self, xn, xm):\n",
    "        return (1.0 + np.dot(xn.T, xm))**self.Q\n",
    "    \n",
    "    def kernel_rbf(self, xn, xm):\n",
    "        gamma = 1.0\n",
    "        xn_square = np.dot(xn.T, xn)\n",
    "        xm_square = np.dot(xm.T, xm)\n",
    "        #return np.exp(-gamma * (xn_square + xm_square - 2 * np.dot(xn.T, xm)))\n",
    "        return np.exp(-gamma * (np.linalg.norm(xn - xm)**2))\n",
    "    \n",
    "    def transform_inputs(self, inputs):\n",
    "        return np.insert(inputs, 0, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute in-sample error and number of support vectors for digit classifiers with C = 0.01 and Q = 2"
   ]
  },
  {
   "cell_type": "code",