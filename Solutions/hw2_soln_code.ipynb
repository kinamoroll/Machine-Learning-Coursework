
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "from scipy.stats import bernoulli\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coin:\n",
    "    \"\"\"Coin class for representing and flipping coins\"\"\"\n",
    "    \n",
    "    def __init__(self, p=0.5):\n",
    "        \"\"\"Create a new coin\n",
    "        Args:\n",
    "        p (float): Probability of flipping a head 0.0 <= p <= 1.0\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "    \n",
    "    def flip(self, n=1):\n",
    "        \"\"\"Flip the coin a number of times independently\n",
    "        Args:\n",
    "        n (int): Number of coin flips\n",
    "        Returns: \n",
    "        np.ndarray where each element is either heads (1) or tails (0)  \n",
    "        \"\"\"\n",
    "        return bernoulli.rvs(self.p, size=n)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fraction_of_heads(coin):\n",
    "        \"\"\"Calculate fraction of heads obtained in coin flips\n",
    "        Args:\n",
    "        coin : Coin object\n",
    "        Returns:\n",
    "        Fraction of heads obtained in coin flips (float) \n",
    "        \"\"\"\n",
    "        return np.mean(coin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coins:\n",
    "    \"\"\"Coins class for representing and flipping coins\"\"\"\n",
    "    \n",
    "    def __init__(self, N=1, p=0.5):\n",
    "        \"\"\"Create a new coin\n",
    "        Args:\n",
    "        p (float): Probability of flipping a head 0.0 <= p <= 1.0\n",
    "        N (int): Number of coins to create\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        self.flips = None\n",
    "    \n",
    "    def flip(self, n=1):\n",
    "        \"\"\"Flip the coins a number of times independently\n",
    "        Args:\n",
    "        n (int): Number of coin flips\n",
    "        Returns: \n",
    "        np.ndarray where each row represents a particular coin and the elements\n",
    "        in each row are either heads (1) or tails (0) \n",
    "        \"\"\"\n",
    "        self.flips = np.stack([Coin(self.p).flip(n) for i in range(0, self.N)])\n",
    "        return self.flips\n",
    "    \n",
    "    def coin_first_head_fraction(self):\n",
    "        \"\"\"Fraction of heads in first coin flipped\n",
    "        Returns: \n",
    "        Fraction of heads for the first coin flipped (float)\n",
    "        \"\"\"\n",
    "        coin = self.flips[0]\n",
    "        return Coin.fraction_of_heads(coin)\n",
    "    \n",
    "    def coin_random_head_fraction(self):\n",
    "        \"\"\"Random coin flipped\n",
    "        Returns: \n",
    "        Fraction of heads for a random coin flipped (float)\n",
    "        \"\"\"\n",
    "        coin = rd.choice(self.flips)\n",
    "        return Coin.fraction_of_heads(coin)\n",
    "    \n",
    "    def coin_min_head_fraction(self):\n",
    "        \"\"\"Fraction of heads in coin with minimum head frequency flipped\n",
    "        Returns: \n",
    "        Fraction of heads for the first coin with the minimum\n",
    "        frequency of heads flipped (float)\n",
    "        \"\"\"\n",
    "        coin = self.flips[np.argmin(self.flips.sum(axis=1), axis=0)]\n",
    "        return Coin.fraction_of_heads(coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average value of ν_1 is equal to  0.4894\n",
      "The average value of ν_rand is equal to  0.49310000000000004\n",
      "The average value of ν_min is equal to  0.039200000000000006\n"
     ]
    }
   ],
   "source": [
    "nu_1 = np.array([])\n",
    "nu_rand = np.array([])\n",
    "nu_min = np.array([])\n",
    "n_trials = 1000\n",
    "for i in range(0, n_trials):\n",
    "    coins = Coins(N=1000, p=0.5)\n",
    "    coins.flip(n=10)\n",
    "    nu_1 = np.append(nu_1, coins.coin_first_head_fraction())\n",
    "    nu_rand = np.append(nu_rand, coins.coin_random_head_fraction())\n",
    "    nu_min = np.append(nu_min, coins.coin_min_head_fraction())\n",
    "\n",
    "print('The average value of ν_1 is equal to ', np.mean(nu_1))\n",
    "print('The average value of ν_rand is equal to ', np.mean(nu_rand))\n",
    "print('The average value of ν_min is equal to ', np.mean(nu_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPoint:\n",
    "    \"\"\" RandomPoint class for representing and manipulating random n-dimensional coordinates. \"\"\"\n",
    "    \n",
    "    def __init__(self, dim=1, lower=0.0, upper=1.0):\n",
    "        \"\"\" Create a new random point assuming a uniform distribution between the upper and lower bounds. \n",
    "        Args:\n",
    "        dim (int): Dimension.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.dim = dim # dimension\n",
    "        self.lower = lower # lower bound\n",
    "        self.upper = upper # upper bound\n",
    "        self.point = np.random.uniform(lower, upper, dim) # point (numpy.ndarray) (half open interval!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetFunction:\n",
    "    \"\"\"TargetFunction class for representing and manipulation random curves in n-dimensional coordinates.\"\"\"\n",
    "    def __init__(self, dim=1, lower=0.0, upper=1.0, line=True):\n",
    "        \"\"\" Create a new random curve assuming a uniform distribution between the upper and lower bounds. \n",
    "        Args:\n",
    "        dim (int): Dimension.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        line (bool): True if requesting to draw a line or false to generate the curve x1**2 + x2**2 - 0.6\n",
    "        \"\"\"\n",
    "        self.line = line\n",
    "        \n",
    "        if self.line:\n",
    "            self.points = [RandomPoint(dim, lower, upper) for i in range(0,2)]\n",
    "        \n",
    "    def sign(self, randomPoint):\n",
    "        return np.sign(self.evaluate(randomPoint))\n",
    "        \n",
    "    def evaluate(self, randomPoint): \n",
    "        if self.line:\n",
    "            x_minus_x1  = randomPoint.point[0] - self.points[0].point[0]\n",
    "            y_minus_y1  = randomPoint.point[1] - self.points[0].point[1]\n",
    "            y2_minus_y1 = self.points[1].point[1] - self.points[0].point[1]\n",
    "            x2_minus_x1 = self.points[1].point[0] - self.points[0].point[0]\n",
    "            return x_minus_x1 * y2_minus_y1 - y_minus_y1 * x2_minus_x1\n",
    "        \n",
    "        x1 = randomPoint.point[0] \n",
    "        x2 = randomPoint.point[1] \n",
    "        return x1**2 + x2**2 - 0.6\n",
    "    \n",
    "    def weights(self):\n",
    "        y1_times_x2 = self.points[0].point[1] * self.points[1].point[0]\n",
    "        x1_times_y2 = self.points[0].point[0] * self.points[1].point[1]\n",
    "        y2_minus_y1 = self.points[1].point[1] - self.points[0].point[1]\n",
    "        x2_minus_x1 = self.points[1].point[0] - self.points[0].point[0] \n",
    "        \n",
    "        return np.array([y1_times_x2 - x1_times_y2, y2_minus_y1, -x2_minus_x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLine:\n",
    "    \"\"\" RandomLine class for representing and manipulating random lines in n-dimensional coordinates. \"\"\"\n",
    "    \n",
    "    def __init__(self, dim=1, lower=0.0, upper=1.0):\n",
    "        \"\"\" Create a new random line assuming a uniform distribution between the upper and lower bounds. \n",
    "        Args:\n",
    "        dim (int): Dimension.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"        \n",
    "        self.points = [RandomPoint(dim, lower, upper) for i in range(0,2)] # inputs (list(RandomPoint))\n",
    "    \n",
    "    def target_function(self, randomPoint):\n",
    "        \"\"\" Evaluate target function using equation of line in symmetric form.\n",
    "        https://math.stackexchange.com/questions/274712/calculate-on-which-side-of-a-straight-line-is-a-given-point-located.\n",
    "        https://en.wikipedia.org/wiki/Linear_equation.\n",
    "        Args:\n",
    "        randomPoint (RandomPoint): Random point.\n",
    "        Returns: \n",
    "        int. +1 for the point being above the line. -1 for the point being below the line. 0 otherwise.     \n",
    "        \"\"\"\n",
    "        x_minus_x1  = randomPoint.point[0] - self.points[0].point[0]\n",
    "        y_minus_y1  = randomPoint.point[1] - self.points[0].point[1]\n",
    "        y2_minus_y1 = self.points[1].point[1] - self.points[0].point[1]\n",
    "        x2_minus_x1 = self.points[1].point[0] - self.points[0].point[0]\n",
    "        \n",
    "        return np.sign(x_minus_x1 * y2_minus_y1 - y_minus_y1 * x2_minus_x1)\n",
    "    \n",
    "    def weights(self):\n",
    "        \"\"\" Evaluate weights of equation of line in symmetric form, i.e. w0 * x0 + w1 * x1 + w2 * x2 = 0\n",
    "        https://math.stackexchange.com/questions/274712/calculate-on-which-side-of-a-straight-line-is-a-given-point-located.\n",
    "        https://en.wikipedia.org/wiki/Linear_equation.\n",
    "        Returns: \n",
    "        np.ndarray consisting of weights of the line [w0, w1, w2]    \n",
    "        \"\"\"\n",
    "        y1_times_x2 = self.points[0].point[1] * self.points[1].point[0]\n",
    "        x1_times_y2 = self.points[0].point[0] * self.points[1].point[1]\n",
    "        y2_minus_y1 = self.points[1].point[1] - self.points[0].point[1]\n",
    "        x2_minus_x1 = self.points[1].point[0] - self.points[0].point[0] \n",
    "        \n",
    "        return np.array([y1_times_x2 - x1_times_y2, y2_minus_y1, -x2_minus_x1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \"\"\" DataSet class for representing and manipulating random datasets. \"\"\"\n",
    "    def __init__(self, N=10, dim=1, lower=0.0, upper=1.0):\n",
    "        \"\"\" Create a new data set assuming a uniform distribution between the \n",
    "            upper and lower bounds. \n",
    "        Args:\n",
    "        N (int): Number of points.\n",
    "        dim (int): Dimension of points.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.inputs = [RandomPoint(dim, lower, upper) for i in range(0,N)] # inputs (list(RandomPoint))\n",
    "        self.outputs = [TargetFunction(2, -1.0, 1.0, True).sign(input) for input in self.inputs] # outputs (list(int))\n",
    "        #self.data = list(zip(self.inputs, self.outputs)) # inputs (list(Tuple(RandomPoint, int)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\" Linear Regression class. \"\"\"\n",
    "    \n",
    "    def __init__(self, trainingSet, nonLinearTransform=False):\n",
    "        \"\"\" Create a new Linear Regression. \n",
    "        Args:\n",
    "        TrainingSet (DataSet): Training set.\n",
    "        nonLinearTransfom (bool): If false, use linear feature vector (1, x1, x2) \n",
    "                                  If true, use non-linear feature vector (1, x1, x2, x1*x2, x1**2, x2**2) \n",
    "        \"\"\"\n",
    "        self.trainingSet = trainingSet\n",
    "        self.nonLinearTransform = nonLinearTransform\n",
    "        self.xn = self.transformPoints(trainingSet.inputs)\n",
    "        \n",
    "    def run(self, targetFunction, fractNoise = 0.0):    \n",
    "        \"\"\"Run of Linear Regression Algorithm.\n",
    "        Args:\n",
    "        targetFunction (RandomLine): Random line\n",
    "        fractNoise (float): Fraction of noise to generate in the output (0.0 <= fracNoise <= 1.0)\n",
    "        Returns:\n",
    "        Tuple(g, yn)\n",
    "        \"\"\"\n",
    "        # calculate outputs and add noise\n",
    "        yn = np.array([targetFunction.sign(input) for input in self.trainingSet.inputs], copy=True) # outputs (list(int))\n",
    "        self.addNoise(yn, fractNoise)\n",
    "                \n",
    "        # calculate pseudo-inverse matrix and return g (weights)\n",
    "        X = self.xn\n",
    "        X_trans = np.transpose(X)\n",
    "        X_dagger = np.matmul(inv(np.matmul(X_trans, X)), X_trans)\n",
    "        return np.matmul(X_dagger, yn), yn\n",
    "    \n",
    "    def Ein(self, weights, yn):\n",
    "        \"\"\"Evaluate fraction of in-sample points which are misclassified\n",
    "        Args:\n",
    "        weights (np.ndarray): g function\n",
    "        yn (np.ndarray): f function (outputs)\n",
    "        Returns:\n",
    "        Fraction of in-sample points which are misclassified (0.0 <= float <= 1.0)\n",
    "        \"\"\"\n",
    "        ein = 0\n",
    "        for i in range(0, len(self.xn)):\n",
    "            if np.sign(np.matmul(np.transpose(weights), self.xn[i])) != yn[i]:\n",
    "                ein += 1\n",
    "        return float(ein / len(self.xn))\n",
    "    \n",
    "    def Eout(self, weights, targetFunction, fractNoise=0.0, npoints=1000):\n",
    "        \"\"\"Evaluate fraction of out-of-sample points which are misclassified\n",
    "        Args:\n",
    "        weights (np.ndarray): g function\n",
    "        targetFunction (TargetFunction): f function\n",
    "        fractNoise (float): Fraction of noise to generate in the output (0.0 <= fracNoise <= 1.0)\n",
    "        npoints (int): number of points to generate\n",
    "        Returns:\n",
    "        Fraction of in-sample points which are misclassified (0.0 <= float <= 1.0)\n",
    "        \"\"\"\n",
    "        # generate random points    \n",
    "        estPoints = [RandomPoint(2, -1.0, 1.0) for i in range(0, npoints)]\n",
    "        transPoints = self.transformPoints(estPoints)\n",
    "        \n",
    "        # compute outputs and add noise\n",
    "        yn = np.array([targetFunction.sign(estPoint) for estPoint in estPoints], copy=True)\n",
    "        self.addNoise(yn, fractNoise)\n",
    "        \n",
    "        # compute Eout\n",
    "        eout = 0\n",
    "        for i in range(0, len(transPoints)):\n",
    "            if np.sign(np.matmul(np.transpose(weights), transPoints[i])) != yn[i]:\n",
    "                eout += 1\n",
    "        return float(eout / len(transPoints))\n",
    "    \n",
    "    def Eout_closest(self, weights, hypotheses, targetFunction, fractNoise=0.0, npoints=1000):\n",
    "        \"\"\"Evaluate fraction of out-of-sample points which are misclassified\n",
    "        Args:\n",
    "        weights (np.ndarray): g function\n",
    "        hypotheses (List(np.ndarray)): potential g functions\n",
    "        targetFunction (TargetFunction): f function\n",
    "        fractNoise (float): Fraction of noise to generate in the output (0.0 <= fracNoise <= 1.0)\n",
    "        npoints (int): number of points to generate\n",
    "        Returns:\n",
    "        Tuple(int, float): Index of hypothesis in hypothesis that is besy match to weights and\n",
    "        the probability of the match\n",
    "        \"\"\"\n",
    "        # generate random points    \n",
    "        estPoints = [RandomPoint(2, -1.0, 1.0) for i in range(0, npoints)]\n",
    "        transPoints = self.transformPoints(estPoints)\n",
    "        \n",
    "        # compute outputs and add noise\n",
    "        #yn = np.array([targetFunction.sign(estPoint) for estPoint in estPoints], copy=True)\n",
    "        #self.addNoise(yn, fractNoise)\n",
    "        \n",
    "        # compute Eout\n",
    "        eouts = np.zeros(len(hypotheses))\n",
    "        for p in range(0, len(transPoints)):\n",
    "            for h in range(0, len(hypotheses)):\n",
    "                if np.sign(np.matmul(np.transpose(hypotheses[h]), transPoints[p])) \\\n",
    "                == np.sign(np.matmul(np.transpose(weights), transPoints[p])):\n",
    "                    eouts[h] += 1\n",
    "    \n",
    "        return np.argmax(eouts), float(np.max(eouts) / len(transPoints))\n",
    "    \n",
    "    # privates\n",
    "    def transformPoints(self, points):\n",
    "        if self.nonLinearTransform:\n",
    "            featureVectors = []\n",
    "            for pnt in points:\n",
    "                x1 = pnt.point[0]\n",
    "                x2 = pnt.point[1]\n",
    "                featureVector = [1.0, x1, x2, x1*x2, x1**2, x2**2]\n",
    "                featureVectors.append(featureVector)\n",
    "            return np.array(featureVectors)\n",
    "        else:\n",
    "            return np.array([np.insert(pnt.point, 0, 1.0) for pnt in points])\n",
    "        \n",
    "    def addNoise(self, outputs, fractNoise):\n",
    "        if fractNoise > 0.0:\n",
    "            idxs = [idx for (idx, value) in rd.sample(list(enumerate(outputs)), int(fractNoise * len(outputs)))]\n",
    "            for idx in idxs:\n",
    "                outputs[idx] = -outputs[idx] # flip the output in fracNoise of output      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLinearRegression(trainingSet, fractNoise=0.0, line=True, nonLinearTransform=False, numRuns=1):\n",
    "    '''Run linear regression algorithm.\n",
    "    Args:\n",
    "    trainingSet (DataSet): Training set.\n",
    "    fracNoise (float): Fraction of noise to generate in the output (0.0 <= fracNoise <= 1.0)\n",
    "    line (bool): True if requesting to draw a line or false to generate the curve x1**2 + x2**2 - 0.6\n",
    "    numRuns (int): number of runs  \n",
    "    Returns:\n",
    "    meanEin (float): mean number of in-sample misclassifications\n",
    "    '''\n",
    "    mean_ein = 0.0\n",
    "    mean_eout = 0.0\n",
    "    for i in range(0, numRuns):\n",
    "        lr = LinearRegression(trainingSet, nonLinearTransform)\n",
    "        targetFunction = TargetFunction(2, -1.0, 1.0, line)\n",
    "        weights, yn = lr.run(targetFunction, fractNoise)\n",
    "        mean_ein += lr.Ein(weights, yn)\n",
    "        mean_eout += lr.Eout(weights, targetFunction, fractNoise, npoints=1000)\n",
    "    \n",
    "    mean_ein = mean_ein / numRuns\n",
    "    mean_eout = mean_eout / numRuns\n",
    "    return mean_ein, mean_eout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of mislassified in-sample points Ein =  0.03826000000000006\n",
      "Fraction of mislassified out-of-sample points Eout =  0.04862099999999999\n"
     ]
    }
   ],
   "source": [
    "mean_Ein, mean_Eout = runLinearRegression(DataSet(N=100, dim=2, lower=-1.0, upper=1.0), line=True, fractNoise=0.0, numRuns=1000)\n",
    "print(\"Fraction of mislassified in-sample points Ein = \", mean_Ein)\n",
    "print(\"Fraction of mislassified out-of-sample points Eout = \", mean_Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLA:\n",
    "    \"\"\" Perceptron Learning Algorithm (PLA) class. \"\"\"\n",
    "    \n",
    "    def __init__(self, initialWeights, trainingSet):\n",
    "        \"\"\" Create a new Perceptron Learning Algorithm (PLA). \n",
    "        Args:\n",
    "        dim (int): Dimension.\n",
    "        lower (float): Lower bound.\n",
    "        upper (float): Upper bound.\n",
    "        \"\"\"\n",
    "        self.initialWeights = initialWeights # initial weights\n",
    "        self.xn = np.array([np.insert(input.point, 0, 1.0) for input in trainingSet.inputs], copy=True) # training set inputs\n",
    "        self.trainingSet = trainingSet\n",
    "        #self.yn = np.array(trainingSet.outputs, copy=True) # training set outputs\n",
    "    \n",
    "    def run(self, targetFunction, maxIterations=100):    \n",
    "        \"\"\"Run of Perceptron Learning Algorithm (PLA).\n",
    "        Returns:\n",
    "        g\n",
    "        \"\"\"        \n",
    "        # calculate outputs\n",
    "        yn = np.array([targetFunction.sign(input) for input in self.trainingSet.inputs], copy=True) # outputs (list(int))\n",
    "        \n",
    "        # calculate target function (weights)\n",
    "        targetWeights = targetFunction.weights()\n",
    "        #print('Target weights ', targetWeights)\n",
    "        \n",
    "        # misclassified points\n",
    "        #misPointIdxs = self.misclassifiedPoints(self.initialWeights, yn)\n",
    "          \n",
    "        # PLA iterations\n",
    "        numIterations = 0\n",
    "        weights = np.copy(self.initialWeights) # initialize weights\n",
    "        converged = False\n",
    "        for i in range(0, maxIterations):\n",
    "            numIterations += 1\n",
    "            #print('Iteration # ', numIterations)\n",
    "            \n",
    "            # misclassified points\n",
    "            misPointIdxs = self.misclassifiedPoints(weights, yn)\n",
    "        \n",
    "            # break out of loop if converged\n",
    "            if len(misPointIdxs) == 0:\n",
    "                converged = True\n",
    "                #print('PLA algorithm converged after ', numIterations, ' iterations')\n",
    "                break\n",
    "            \n",
    "            # pick misclassified point at random\n",
    "            misPointIdx = rd.choice(misPointIdxs)\n",
    "            #print('Chosen misclassified point index ', misPointIdx)\n",
    "            \n",
    "            # update weights\n",
    "            weights = np.add(weights, np.multiply(yn[misPointIdx], self.xn[misPointIdx]))\n",
    "            #print('Weights ', weights)\n",
    "            \n",
    "        if not converged:\n",
    "            raise Exception('Perceptron algorithm failed to converge after maximum number of iterations.')\n",
    "        \n",
    "        # estimate disagreement probability between f and g\n",
    "        disagreementProb = self.disagreementProbability(1000, weights, targetWeights)\n",
    "        \n",
    "        return weights, numIterations, disagreementProb\n",
    "    \n",
    "    # private methods\n",
    "    def misclassifiedPoints(self, weights, yn):\n",
    "        misPointIdxs = []      \n",
    "        for i in range(0, len(self.xn)):\n",
    "            if np.sign(np.dot(weights, self.xn[i])) != yn[i]:\n",
    "                misPointIdxs.append(i)\n",
    "        #print('Misclassified point indexes', misPointIdxs)\n",
    "        return misPointIdxs\n",
    "    \n",
    "    def disagreementProbability(self, npoints, weights, targetWeights):\n",
    "        # generate random points\n",
    "        estPoints = [RandomPoint(2, -1.0, 1.0) for i in range(0,npoints)]\n",
    "        points = np.array([np.insert(estPoint.point, 0, 1.0) for estPoint in estPoints])\n",
    "        \n",
    "        # calculate probability\n",
    "        disagreementProb = 0\n",
    "        for point in points:\n",
    "            g = np.sign(weights[0] * point[0] + weights[1] * point[1] + weights[2] * point[2])\n",
    "            f = np.sign(targetWeights[0] * point[0] + targetWeights[1] * point[1] + targetWeights[2] * point[2])\n",
    "            if g != f:\n",
    "                disagreementProb += 1\n",
    "                \n",
    "        return float(disagreementProb / npoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerceptron(initialWeights, trainingSet, numRuns=1, maxIterations=100):\n",
    "    '''Run perceptron algorithm.\n",
    "    Args: \n",
    "    numRuns (int): number of runs\n",
    "    \n",
    "    Returns:\n",
    "    meanIterations (float): mean number of iterations for convergence\n",
    "    '''\n",
    "    meanIterations = 0\n",
    "    meanDisagreementProb = 0.0\n",
    "    for i in range(0, numRuns):\n",
    "        pla = PLA(initialWeights, trainingSet)\n",
    "        targetFunction = TargetFunction(2, -1.0, 1.0, True)\n",
    "        weights, numIterations, disagreementProb = pla.run(targetFunction, maxIterations)\n",
    "        #print('Final weights ', weights)\n",
    "        meanIterations += numIterations\n",
    "        meanDisagreementProb += disagreementProb\n",
    "    \n",
    "    meanIterations = meanIterations / numRuns\n",
    "    meanDisagreementProb = meanDisagreementProb / numRuns\n",
    "    \n",
    "    return meanIterations, meanDisagreementProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPerceptronWithLinearRegressionInitialWeights(trainingSet, numRuns=1, maxIterations=100):\n",
    "    '''Run perceptron algorithm using linear regression for initial weights\n",
    "    Args: \n",
    "    numRuns (int): number of runs\n",
    "    \n",
    "    Returns:\n",
    "    meanIterations (float): mean number of iterations for convergence\n",
    "    '''\n",
    "    meanIterations = 0\n",
    "    for i in range(0, numRuns):\n",
    "        lr = LinearRegression(trainingSet, False) # run linear regression\n",
    "        targetFunction = TargetFunction(2, -1.0, 1.0, True)\n",
    "        initialWeights, yn = lr.run(targetFunction)\n",
    "        pla = PLA(initialWeights, trainingSet)\n",
    "        weights, numIterations, disagreementProb = pla.run(targetFunction, maxIterations)\n",
    "        meanIterations += numIterations\n",
    "    \n",
    "    meanIterations = meanIterations / numRuns   \n",
    "    return meanIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations =  5.042\n"
     ]
    }
   ],
   "source": [
    "trainingSet = DataSet(N=10, dim=2, lower=-1.0, upper=1.0)\n",
    "meanIterations = runPerceptronWithLinearRegressionInitialWeights(trainingSet, numRuns=1000, maxIterations=1000)\n",
    "print('Average number of iterations = ', meanIterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonlinear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLinearRegressionHypotheses(testHypotheses, trainingSet, fractNoise=0.0, line=True, nonLinearTransform=False, numRuns=1):\n",
    "    '''Run linear regression algorithm comparing against hypotheses.\n",
    "    Args:\n",
    "    testHypotheses (dict(name, weights)) Test hypothesis\n",
    "    trainingSet (DataSet): Training set.\n",
    "    fracNoise (float): Fraction of noise to generate in the output (0.0 <= fracNoise <= 1.0)\n",
    "    line (bool): True if requesting to draw a line or false to generate the curve x1**2 + x2**2 - 0.6\n",
    "    numRuns (int): number of runs  \n",
    "    Returns:\n",
    "    dict(name, agreementProb): Name and agreement probability of the best hypothesis\n",
    "    '''\n",
    "    meanProb = 0.0\n",
    "    hyps = []\n",
    "    for i in range(0, numRuns):\n",
    "        lr = LinearRegression(trainingSet, nonLinearTransform)\n",
    "        targetFunction = TargetFunction(2, -1.0, 1.0, line)\n",
    "        weights, yn = lr.run(targetFunction, fractNoise)\n",
    "        hyp, prob = lr.Eout_closest(weights, testHypotheses, targetFunction, fractNoise, npoints=1000)\n",
    "        hyps.append(hyp) # always same value\n",
    "        meanProb += prob\n",
    "    \n",
    "    return hyps[0], meanProb / numRuns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closes hypothesis is  [-1.   -0.05  0.08  0.13  1.5   1.5 ] with a mean probability of  0.9642000000000002\n"
     ]
    }
   ],
   "source": [
    "testHypotheses = [\n",
    "    np.array([-1.0, -0.05, 0.08, 0.13, 1.5, 1.5]),\n",
    "    np.array([-1.0, -0.05, 0.08, 0.13, 1.5, 15.0]),\n",
    "    np.array([-1.0, -0.05, 0.08, 0.13, 15.0, 1.5]),\n",
    "    np.array([-1.0, -1.5, 0.08, 0.13, 0.05, 0.05]),\n",
    "    np.array([-1.0, -0.05, 0.08, 1.5, 0.15, 0.15])\n",
    "]\n",
    "\n",
    "hyp, meanProb = (runLinearRegressionHypotheses(testHypotheses, DataSet(N=1000, dim=2, lower=-1.0, upper=1.0), line=False, \n",
    "                                               nonLinearTransform=True, fractNoise=0.1, numRuns=10))\n",
    "\n",
    "print('The closes hypothesis is ', testHypotheses[hyp], 'with a mean probability of ', meanProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of mislassified out-of-sample points Eout =  0.11723099999999965\n"
     ]
    }
   ],
   "source": [
    "mean_Ein, mean_Eout = (runLinearRegression(DataSet(N=1000, dim=2, lower=-1.0, upper=1.0), line=False, \n",
    "                                           nonLinearTransform=True, fractNoise=0.1, numRuns=1000))\n",
    "print(\"Fraction of mislassified out-of-sample points Eout = \", mean_Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}