
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "# CVXOPT runs extremely slowly for datasets in the 1000's of instances. As the Professor suggests in the homework, Libsvm is\n",
    "# highly performant. In fact libsvm is used under the hood in sklearn and hence the use of this library instead.\n",
    "\n",
    "# Follow instructions here to get cvxopt working. --- painful!!! \n",
    "#https://stackoverflow.com/questions/46009925/how-to-install-cvxopt-on-on-windows-10-on-python-3-6\n",
    "#from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training and testing data in pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    column_names = ['digit', 'intensity', 'symmetry']\n",
    "    sep = '\\s+'\n",
    "    features_train = pd.read_table('http://www.amlbook.com/data/zip/features.train', sep=sep, names=column_names)\n",
    "    features_test = pd.read_table('http://www.amlbook.com/data/zip/features.test', sep=sep, names=column_names)\n",
    "    return features_train, features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set output labels for one-vs-all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_one_vs_all = dict(\n",
    "    {'zero_vs_all': 0,\n",
    "     'one_vs_all': 1,\n",
    "     'two_vs_all': 2,\n",
    "     'three_vs_all': 3,\n",
    "     'four_vs_all': 4,\n",
    "     'five_vs_all': 5,\n",
    "     'six_vs_all': 6,\n",
    "     'seven_vs_all': 7,\n",
    "     'eight_vs_all': 8,\n",
    "     'nine_vs_all': 9\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one-versus-all dataframe by adding outputs for the different classifiers to the training dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_vs_all_dataframe(df, classifiers):\n",
    "    # add binary labels to dataframe\n",
    "    one_vs_all = pd.DataFrame(df, copy=True)\n",
    "    for class_label, digit in classifiers.items():\n",
    "        labels = one_vs_all.loc[one_vs_all['digit'] == digit, 'digit']\n",
    "        labels.loc[:] = 1.0\n",
    "        one_vs_all[class_label] = labels\n",
    "        \n",
    "    one_vs_all.fillna(-1.0, inplace=True)\n",
    "    return one_vs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_one_vs_all = create_one_vs_all_dataframe(features_train, classifiers_one_vs_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dataset (inputs / outputs) for a specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(df, class_label):\n",
    "    inputs = np.array(df.loc[:, ['intensity', 'symmetry']])\n",
    "    outputs = np.array(df.loc[:, class_label])\n",
    "    data = np.column_stack((inputs, outputs))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Support Vector Machine (SVM) class - this is not used due to how slow quadratic programming package is - left here for illustration of the general structure of algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\" Support Vector Machine (SVM) class. \"\"\"\n",
    "    \n",
    "    def __init__(self, Q=2, rbf=False):\n",
    "        \"\"\" Create a Support Vector Machine Algorithm (SVM). \n",
    "        Args:\n",
    "        Q (int): Degree of the polynomial kernel (Q >= 0)\n",
    "        rbf (bool): Use the RBF kernel instead of the polynomial kernel\n",
    "        \"\"\"\n",
    "        self.Q = Q\n",
    "        self.rbf = rbf\n",
    "        \n",
    "    def train(self, inputs, outputs, C=0.01, margin='hard'):    \n",
    "        \"\"\"Training using Support Vector Machine Algorithm (SVM).\n",
    "        Args:\n",
    "        inputs (np.ndarray): Input points.\n",
    "        outputs (np.ndarray): Targets.\n",
    "        C (float): Upper bound of Lagrange multiplier (C > 0.0)\n",
    "        margin (string): Margin to apply, 'hard' or 'soft'\n",
    "        Returns:\n",
    "        Tuple(np.ndarray, int): Weights, number of support vectors\n",
    "        \"\"\"\n",
    "        if margin not in ['hard','soft']:\n",
    "            raise ValueError('Margin must be \"hard\" or \"soft\".')\n",
    "        \n",
    "        xn = inputs\n",
    "        yn = outputs\n",
    "        N = len(xn)\n",
    "        \n",
    "        mat = []\n",
    "        for row_idx in range(0, N):\n",
    "            for col_idx in range(0, N):\n",
    "                if self.rbf:\n",
    "                    kernel = self.kernel_rbf(xn[row_idx], xn[col_idx])\n",
    "                else:\n",
    "                    kernel = self.kernel_poly(xn[row_idx], xn[col_idx])\n",
    "                val = yn[row_idx] * yn[col_idx] * kernel\n",
    "                mat.append(val)\n",
    "        mat = np.array(mat).reshape((N, N))\n",
    "        \n",
    "        # form matrices for quadratic programming solver\n",
    "        dim = len(xn[0])\n",
    "        P = matrix(mat, tc='d')\n",
    "        q = matrix(-np.ones(N), tc='d')\n",
    "        b = matrix(0.0, tc='d')\n",
    "        A = matrix(yn, tc='d')\n",
    "        A = A.trans()\n",
    "        G = matrix(-np.identity(N), tc='d')\n",
    "        if margin == 'hard':\n",
    "            G = matrix(-np.identity(N), tc='d')\n",
    "            h = matrix(np.zeros(N), tc='d')\n",
    "        elif margin == 'soft':\n",
    "            G_zero = -np.identity(N)\n",
    "            h_zero = np.zeros(N)\n",
    "            G_C = np.identity(N)\n",
    "            h_C = C * np.ones(N)\n",
    "              \n",
    "            G = matrix(np.concatenate((G_zero, G_C)), tc='d')\n",
    "            h = matrix(np.concatenate((h_zero, h_C)), tc='d')\n",
    "        \n",
    "        #print('P = ', P)\n",
    "        #print('q = ', q)\n",
    "        #print('G = ', G)\n",
    "        #print('h = ', h)\n",
    "        #print('A = ', A)\n",
    "        #print('b = ', b)\n",
    "                \n",
    "        # call qp solver to compute weights\n",
    "        solvers.options['show_progress'] = False # supress solver output\n",
    "        \n",
    "        sol = solvers.qp(P, q, G, h, A, b)\n",
    "        alpha = np.array(list(sol['x']))\n",
    "        #print('alpha = ', sol['x'])\n",
    "        \n",
    "        weights = np.zeros(dim)\n",
    "        #sv_idxs = []\n",
    "        sv = []\n",
    "        sv_alphas = []\n",
    "        sv_outputs = []\n",
    "        for n in range(0, N):\n",
    "            if margin == 'hard':\n",
    "                weights += alpha[n] * yn[n] * xn[n]\n",
    "                if alpha[n] > 1e-5: # => xn[n] is support vector\n",
    "                    sv.append(xn[n])\n",
    "            elif margin == 'soft':\n",
    "                #print('alpha[{0}] = {1}'.format(n, alpha[n]))\n",
    "                if alpha[n] > 1e-5 and alpha[n] <= C: # => xn[n] is support vector\n",
    "                    #sv_idxs.append(n)\n",
    "                    sv.append(xn[n])\n",
    "                    sv_alphas.append(alpha[n])\n",
    "                    sv_outputs.append(yn[n])\n",
    "                    \n",
    "        # compute number of support vectors\n",
    "        num_sv = len(sv)\n",
    "        \n",
    "        if (num_sv == 0):\n",
    "            raise Exception('There are no support vectors.')\n",
    "        \n",
    "        bs = []\n",
    "        for m in range(0, num_sv):\n",
    "            if margin == 'hard':\n",
    "                b = (1. / sv_outputs[m]) - np.dot(weights.T, sv[m])\n",
    "            elif margin == 'soft':\n",
    "                b = sv_outputs[m]\n",
    "                for n in range(0, num_sv):\n",
    "                    if self.rbf:\n",
    "                        kernel = self.kernel_rbf(sv[n], sv[m])\n",
    "                    else:\n",
    "                        kernel = self.kernel_poly(sv[n], sv[m])\n",
    "                    b -= sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            bs.append(b)\n",
    "        bs_round = np.round(bs, 1)\n",
    "        #print('bs sv = ', bs)\n",
    "        \n",
    "        #print('bs = ', bs)\n",
    "        #print('bs_round = ', bs_round)\n",
    "        #print('np.unique(bs_round) = ', np.unique(bs_round))\n",
    "        \n",
    "        #if (len(np.unique(bs_round)) != 1):\n",
    "        #    raise Exception('All support vectors must produce the same value of b.')\n",
    "            \n",
    "        weights = np.insert(weights, 0, b)\n",
    "        #print('weights = ', weights)\n",
    "\n",
    "        if margin == 'hard':\n",
    "            return weights, num_sv\n",
    "        elif margin == 'soft':\n",
    "            return np.array(sv_alphas), np.array(sv), np.array(sv_outputs), b\n",
    "    \n",
    "    def binary_error(self, sv_alphas, sv, sv_outputs, b, inputs, outputs):\n",
    "        \"\"\" Evaluate binary classification error. \n",
    "        Args:\n",
    "        sv_alphas (np.ndarray): Support vector Lagrange multipliers.\n",
    "        sv (np.ndarray): Support vectors.\n",
    "        sv_outputs (np.ndarray): Support vector outputs.\n",
    "        b (float): Constant.\n",
    "        inputs (np.ndarray): Inputs.\n",
    "        outputs (np.ndarray): Outputs.\n",
    "        Returns (float): Binary classification error percentage\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        y = outputs\n",
    "        num_sv = len(sv)\n",
    "        \n",
    "        gs = []\n",
    "        for xm in x:\n",
    "            signal = 0.0\n",
    "            for n in range(0, num_sv):\n",
    "                if self.rbf:\n",
    "                    kernel = self.kernel_rbf(sv[n], xm)\n",
    "                else:\n",
    "                    kernel = self.kernel_poly(sv[n], xm)\n",
    "                signal += sv_alphas[n] * sv_outputs[n] * kernel\n",
    "            signal += b\n",
    "            gs.append(signal)\n",
    "                \n",
    "        g = np.array(np.sign(gs))\n",
    "        return 100. * np.sum(y != g) / len(y)     \n",
    "    \n",
    "    # privates\n",
    "    def kernel_poly(self, xn, xm):\n",
    "        return (1.0 + np.dot(xn.T, xm))**self.Q\n",
    "    \n",
    "    def kernel_rbf(self, xn, xm):\n",
    "        gamma = 1.0\n",
    "        xn_square = np.dot(xn.T, xn)\n",
    "        xm_square = np.dot(xm.T, xm)\n",
    "        #return np.exp(-gamma * (xn_square + xm_square - 2 * np.dot(xn.T, xm)))\n",
    "        return np.exp(-gamma * (np.linalg.norm(xn - xm)**2))\n",
    "    \n",
    "    def transform_inputs(self, inputs):\n",
    "        return np.insert(inputs, 0, 1.0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute in-sample error and number of support vectors for digit classifiers with C = 0.01 and Q = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " def in_out_sample_errors(classifiers, features_train, features_test=None, Cs=[0.01], Qs=[2], kernel='poly', \n",
    "                          gamma=1.0, coef0=1.0):\n",
    "    for class_label in classifiers.keys():\n",
    "        for C in Cs:\n",
    "            for Q in Qs:\n",
    "                dataset_train = get_dataset(features_train, class_label)\n",
    "                inputs_train = dataset_train[:, 0:2]\n",
    "                outputs_train = dataset_train[:, 2]\n",
    "                \n",
    "                if kernel == 'poly':\n",
    "                    clf = svm.SVC(C=C, kernel=kernel, gamma=gamma, coef0=coef0, degree=Q)\n",
    "                elif kernel == 'rbf':\n",
    "                    clf = svm.SVC(C=C, kernel=kernel, gamma=gamma)\n",
    "                clf.fit(inputs_train, outputs_train)\n",
    "                \n",
    "                error_in_sample = 1.0 - clf.score(inputs_train, outputs_train)\n",
    "                \n",
    "                if not features_test.empty:\n",
    "                    dataset_test = get_dataset(features_test, class_label)\n",
    "                    inputs_test = dataset_test[:, 0:2]\n",
    "                    outputs_test = dataset_test[:, 2]\n",
    "                    \n",
    "                    error_out_sample = 1.0 - accuracy_score(outputs_test, clf.predict(inputs_test))\n",
    "                \n",
    "                if features_test.empty:\n",
    "                     print('For C={0} and the polynomial kernel (Q={1}), the \"{2}\" classifier in-sample error Ein = {3}%, '\n",
    "                           'and number of support vectors = {4}.'\n",
    "                           .format(C, Q, class_label, round(error_in_sample, 3), len(clf.support_vectors_)))\n",
    "                elif kernel == 'poly':\n",
    "                    print('For C={0} and the polynomial kernel (Q={1}), the \"{2}\" classifier in-sample error Ein = {3}%, ' \n",
    "                          'out-of-sample error Eout = {4}% and number of support vectors = {5}.'\n",
    "                          .format(C, Q, class_label, round(error_in_sample, 3), round(error_out_sample, 3), \n",
    "                                  len(clf.support_vectors_)))\n",
    "                elif kernel == 'rbf':\n",
    "                    print('For C={0}, and the RBF kernel (γ={1}), the \"{2}\" classifier in-sample error Ein = {3}% and ' \n",
    "                          'out-of-sample error Eout = {4}%'\n",
    "                          .format(C, gamma, class_label, round(error_in_sample, 3), round(error_out_sample, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.01 and the polynomial kernel (Q=2), the \"zero_vs_all\" classifier in-sample error Ein = 0.106%, and number of support vectors = 2179.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"one_vs_all\" classifier in-sample error Ein = 0.014%, and number of support vectors = 386.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"two_vs_all\" classifier in-sample error Ein = 0.1%, and number of support vectors = 1970.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"three_vs_all\" classifier in-sample error Ein = 0.09%, and number of support vectors = 1950.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"four_vs_all\" classifier in-sample error Ein = 0.089%, and number of support vectors = 1856.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"five_vs_all\" classifier in-sample error Ein = 0.076%, and number of support vectors = 1585.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"six_vs_all\" classifier in-sample error Ein = 0.091%, and number of support vectors = 1893.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"seven_vs_all\" classifier in-sample error Ein = 0.088%, and number of support vectors = 1704.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"eight_vs_all\" classifier in-sample error Ein = 0.074%, and number of support vectors = 1776.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"nine_vs_all\" classifier in-sample error Ein = 0.088%, and number of support vectors = 1978.\n"
     ]
    }
   ],
   "source": [
    "in_out_sample_errors(classifiers_one_vs_all, features_train_one_vs_all, pd.DataFrame(), Cs=[0.01], Qs=[2], kernel='poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set output labels for one-vs-one classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_one_vs_one = dict({'one_vs_five': [1,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one-versus-one dataframe by adding outputs for the different classifiers to the training and testing dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_vs_one_dataframe(df, classifiers):\n",
    "    # add binary labels to dataframe  \n",
    "    for class_label in classifiers.keys():\n",
    "        digits = classifiers[class_label]\n",
    "        one_vs_one = pd.DataFrame(df.loc[df['digit'].isin(digits),:], copy=True)\n",
    "        for digit in digits:\n",
    "            labels = one_vs_one.loc[one_vs_one['digit'] == digit, 'digit']\n",
    "            labels.loc[:] = 1.0\n",
    "            one_vs_one[class_label] = labels\n",
    "            break\n",
    "        \n",
    "    one_vs_one.fillna(-1.0, inplace=True)\n",
    "    return one_vs_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_one_vs_one = create_one_vs_one_dataframe(features_train, classifiers_one_vs_one)\n",
    "features_test_one_vs_one = create_one_vs_one_dataframe(features_test, classifiers_one_vs_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute in-sample error, out-of-sample error and number of support vectors for 1 vs 5 classifier with C = {0.001, 0.01, 0.1, 1} and Q = {2, 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.001 and the polynomial kernel (Q=2), the \"one_vs_five\" classifier in-sample error Ein = 0.004%, out-of-sample error Eout = 0.017% and number of support vectors = 76.\n",
      "For C=0.001 and the polynomial kernel (Q=5), the \"one_vs_five\" classifier in-sample error Ein = 0.004%, out-of-sample error Eout = 0.021% and number of support vectors = 25.\n",
      "For C=0.01 and the polynomial kernel (Q=2), the \"one_vs_five\" classifier in-sample error Ein = 0.004%, out-of-sample error Eout = 0.019% and number of support vectors = 34.\n",
      "For C=0.01 and the polynomial kernel (Q=5), the \"one_vs_five\" classifier in-sample error Ein = 0.004%, out-of-sample error Eout = 0.021% and number of support vectors = 23.\n",
      "For C=0.1 and the polynomial kernel (Q=2), the \"one_vs_five\" classifier in-sample error Ein = 0.004%, out-of-sample error Eout = 0.019% and number of support vectors = 24.\n",
      "For C=0.1 and the polynomial kernel (Q=5), the \"one_vs_five\" classifier in-sample error Ein = 0.003%, out-of-sample error Eout = 0.019% and number of support vectors = 25.\n",
      "For C=1.0 and the polynomial kernel (Q=2), the \"one_vs_five\" classifier in-sample error Ein = 0.003%, out-of-sample error Eout = 0.019% and number of support vectors = 24.\n",
      "For C=1.0 and the polynomial kernel (Q=5), the \"one_vs_five\" classifier in-sample error Ein = 0.003%, out-of-sample error Eout = 0.021% and number of support vectors = 21.\n"
     ]
    }
   ],
   "source": [
    "in_out_sample_errors(classifiers_one_vs_one, features_train_one_vs_one, features_test_one_vs_one, \n",
    "                     Cs=[0.001, 0.01, 0.1, 1.0], Qs=[2, 5], kernel='poly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 10-fold cross-validation error Ecv and the value of C with the lowest Ecv over 100 runs for 1 vs 5 classifier with C = {0.0001, 0.001, 0.01, 0.1, 1} and Q = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_error(classifiers, features_train, Cs, num_folds, shuffle, num_runs):\n",
    "    errors_cv = OrderedDict.fromkeys(Cs, 0.0)\n",
    "    low_cv_counts = OrderedDict.fromkeys(Cs, 0)\n",
    "    \n",
    "    for class_label in classifiers.keys():\n",
    "        dataset_train = get_dataset(features_train, class_label)\n",
    "        inputs_train = dataset_train[:, 0:2]\n",
    "        outputs_train = dataset_train[:, 2]\n",
    "        \n",
    "        for i in range(0, num_runs):\n",
    "            k_fold = KFold(n_splits=num_folds, shuffle=shuffle)\n",
    "            run_errors_cv = []\n",
    "            for C in Cs:\n",
    "                clf = svm.SVC(C=C, kernel='poly', gamma=1.0, coef0=1.0, degree=2)\n",
    "                scores = cross_val_score(clf, inputs_train, outputs_train, cv=k_fold)\n",
    "                #print('scores = ', scores)\n",
    "                error_cv = 1.0 - scores.mean()\n",
    "                errors_cv[C] += error_cv\n",
    "                run_errors_cv.append(error_cv)\n",
    "            \n",
    "            min_error_cv = np.argmin(run_errors_cv)\n",
    "            low_cv_counts[list(low_cv_counts.keys())[min_error_cv]] += 1\n",
    "            \n",
    "            run_num = i+1\n",
    "            #print('Run number {0} completed, C = {1} selected, error = {2}'\n",
    "            #      .format(run_num, list(low_cv_counts.keys())[min_error_cv], run_errors_cv[min_error_cv]))\n",
    "\n",
    "        low_cv_count = max(low_cv_counts, key=low_cv_counts.get)\n",
    "            \n",
    "        print('C={0} is selected the most often ({1} times out of {2} runs) and the average value of the cross-validation'\n",
    "              ' error Ecv = {3}'.format(low_cv_count, low_cv_counts[low_cv_count], num_runs, \n",
    "                                        round(errors_cv[low_cv_count] / num_runs, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.001 is selected the most often (28 times out of 100 runs) and the average value of the cross-validation error Ecv = 0.005\n"
     ]
    }
   ],
   "source": [
    "cross_val_error(classifiers_one_vs_one, features_train_one_vs_one, Cs=[0.0001, 0.001, 0.01, 0.1, 1.0], num_folds=10, \n",
    "                shuffle=True, num_runs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute in-sample error, out-of-sample error and number of support vectors for 1 vs 5 classifier with C = {0.01, 1, 100, 10^4, 10^6} for radial basis function (RBF kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For C=0.01, and the RBF kernel (γ=1.0), the \"one_vs_five\" classifier in-sample error Ein = 0.004% and out-of-sample error Eout = 0.024%\n",
      "For C=1.0, and the RBF kernel (γ=1.0), the \"one_vs_five\" classifier in-sample error Ein = 0.004% and out-of-sample error Eout = 0.021%\n",
      "For C=100.0, and the RBF kernel (γ=1.0), the \"one_vs_five\" classifier in-sample error Ein = 0.003% and out-of-sample error Eout = 0.019%\n",
      "For C=10000.0, and the RBF kernel (γ=1.0), the \"one_vs_five\" classifier in-sample error Ein = 0.003% and out-of-sample error Eout = 0.024%\n",
      "For C=1000000.0, and the RBF kernel (γ=1.0), the \"one_vs_five\" classifier in-sample error Ein = 0.001% and out-of-sample error Eout = 0.024%\n"
     ]
    }
   ],
   "source": [
    "in_out_sample_errors(classifiers_one_vs_one, features_train_one_vs_one, features_test_one_vs_one, \n",
    "                     Cs=[0.01, 1., 100., 1.*10**4, 1.*10**6], kernel='rbf', gamma=1.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}