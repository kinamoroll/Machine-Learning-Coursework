
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import inv\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Follow instructions here to get cvxopt working. --- painful!!! \n",
    "#https://stackoverflow.com/questions/46009925/how-to-install-cvxopt-on-on-windows-10-on-python-3-6\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\" Linear Regression class. \"\"\"\n",
    "    \n",
    "    def __init__(self, transform=True):\n",
    "        \"\"\"Initializer\n",
    "        Args:\n",
    "        transform (bool): Apply non-linear transformation to the data\n",
    "        k (int): Dimensionality of non-linear transformation, 3 <= k <= 7\n",
    "        lambda (float): Regularization parameter\n",
    "        Returns:\n",
    "        np.ndarray: Weights\n",
    "        \"\"\"           \n",
    "        self.non_linear = transform \n",
    "    \n",
    "    def train(self, train_data, k=3, const_hypothesis=False, llambda=0.0):    \n",
    "        \"\"\"Train the Linear Regression Algorithm\n",
    "        Args:\n",
    "        train_data (np.ndarray): Training data\n",
    "        k (int): Dimensionality of non-linear transformation, 3 <= k <= 7\n",
    "        const_hypothesis (bool): If True, use constant hypothesis h(x) = b, \n",
    "                                 If False, use linear hypothesis h(x) = ax + b\n",
    "        lambda (float): Regularization parameter\n",
    "        Returns:\n",
    "        np.ndarray: Weights\n",
    "        \"\"\"      \n",
    "        # apply non-linear transformation to inputs\n",
    "        inputs = train_data[:, 0:np.size(train_data, axis=1) - 1]\n",
    "        Z = self.transform_inputs(inputs, k) # transform inputs\n",
    "        \n",
    "        # get outputs\n",
    "        y = train_data[:, np.size(train_data, axis=1) - 1]\n",
    "        \n",
    "        # linear regression solution with regularization\n",
    "        if const_hypothesis:\n",
    "            weights = np.mean(y)\n",
    "        else:\n",
    "            Z_trans = np.transpose(Z)\n",
    "            normZ = np.matmul(Z_trans, Z)  \n",
    "            I = np.identity(np.size(normZ, 0))\n",
    "            weights = np.matmul(inv(np.add(normZ, llambda * I)), np.matmul(Z_trans, y))\n",
    "        return weights\n",
    "    \n",
    "    def sample_error(self, weights, data, k=3):    \n",
    "        \"\"\"Compute sample classification error\n",
    "        Args:\n",
    "        weights (np.ndarray): Weights of hypothesis\n",
    "        data (np.ndarray): Training/validation/testing data\n",
    "        k (int): Dimensionality of non-linear transformation, 3 <= k <= 7\n",
    "        Returns:\n",
    "        float: Sample error (fraction of misclassified points)\n",
    "        \"\"\"\n",
    "        z = self.transform_inputs(data[:, 0:np.size(data, axis=1) - 1], k)\n",
    "        y = data[:, np.size(data, axis=1) - 1]\n",
    "        \n",
    "        num_errors = 0\n",
    "        for n in range(0, len(z)):\n",
    "            #print('weights.T) = ', weights.T)\n",
    "            #print('z[n] = ', z[n])\n",
    "            #print('y[n] = ', y[n])\n",
    "            if np.sign(np.dot(weights.T, z[n])) != y[n]:\n",
    "               num_errors += 1\n",
    "        return num_errors / len(z)\n",
    "    \n",
    "    def cross_val_error(self, data, const_hypothesis=False, k=3):\n",
    "        \"\"\"Compute cross-validation error using squared error\n",
    "        Args:\n",
    "        data (np.ndarray): Data\n",
    "        const_hypothesis (bool): If True, use constant hypothesis h(x) = b, \n",
    "                                 If False, use linear hypothesis h(x) = ax + b\n",
    "        k (int): Dimensionality of non-linear transformation, 3 <= k <= 7\n",
    "        Returns:\n",
    "        float: Cross-validation error (using squared error)\n",
    "        \"\"\"\n",
    "        #print('data = ', data)\n",
    "        squared_errors = []\n",
    "        loo = LeaveOneOut()\n",
    "        for train_ind, val_ind in loo.split(data):\n",
    "            #print('=========================================================')\n",
    "            train_set = data[train_ind]\n",
    "            val_set = data[val_ind]\n",
    "            #print('train_set = ', train_set)\n",
    "            #print('val_set = ', val_set)\n",
    "            weights = self.train(train_set, k, const_hypothesis)\n",
    "            \n",
    "            #print('weights = ', weights) \n",
    "            \n",
    "            #z = self.transform_inputs(data[:, 0:np.size(data, axis=1) - 1], k)\n",
    "            #y = data[:, np.size(data, axis=1) - 1]\n",
    "            z = self.transform_inputs(val_set, k)\n",
    "            y = val_set[:, np.size(val_set, axis=1) - 1]\n",
    "            #print('z = ', z)\n",
    "            #print('y = ', y)\n",
    "\n",
    "            squared_errors.append((np.dot(weights.T, z[0]) - y[0])**2)\n",
    "        \n",
    "        return np.mean(squared_errors)       \n",
    "         \n",
    "    # privates\n",
    "    def transform_inputs(self, inputs, k):\n",
    "        ones = np.ones(len(inputs)).reshape(len(inputs), 1)       \n",
    "        if self.non_linear:      \n",
    "            x1 = inputs[:, 0].reshape(len(inputs), 1)\n",
    "            x2 = inputs[:, 1].reshape(len(inputs), 1)\n",
    "            transform = (ones, x1, x2, x1**2, x2**2, x1*x2, np.abs(x1-x2), np.abs(x1+x2))\n",
    "        else:\n",
    "            x1 = inputs[:, 0].reshape(len(inputs), 1)\n",
    "            transform = (ones, x1)\n",
    "        trans_inputs = np.concatenate(transform[:k+1], axis=1)\n",
    "        #print('trans_inputs = ', trans_inputs)\n",
    "        return trans_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute validation and out-of-sample (test) errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_test_error(train_set, val_set, test_set):\n",
    "    low_k_val = 2\n",
    "    low_k_out = 2\n",
    "    low_error_val = 999\n",
    "    low_error_out = 999\n",
    "    for k in range(3, 8):\n",
    "        lr = LinearRegression()\n",
    "        weights = lr.train(train_set, k)\n",
    "        error_val = lr.sample_error(weights, val_set, k)\n",
    "        error_out = lr.sample_error(weights, test_set, k)\n",
    "        if error_val < low_error_val:\n",
    "            low_error_val = error_val\n",
    "            low_k_val = k\n",
    "        if error_out < low_error_out:\n",
    "            low_error_out = error_out\n",
    "            low_k_out = k\n",
    "    print('The lowest validation error is {0} occuring at k = {1}'.format(round(low_error_val, 1), low_k_val))\n",
    "    print('The lowest out-of-sample (test) error is {0} occuring at k = {1}'.format(round(low_error_out, 1), low_k_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in.dta and out.dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dta = np.array(\n",
    "[ -7.7947021e-01,   8.3822138e-01,   1.0000000e+00,\n",
    "   1.5563491e-01,   8.9537743e-01,   1.0000000e+00,\n",
    "  -5.9907703e-02,  -7.1777995e-01,   1.0000000e+00,\n",
    "   2.0759636e-01,   7.5893338e-01,   1.0000000e+00,\n",
    "  -1.9598312e-01,  -3.7548716e-01,  -1.0000000e+00,\n",
    "   5.8848947e-01,  -8.4255381e-01,   1.0000000e+00,\n",
    "   7.1985874e-03,  -5.4831650e-01,  -1.0000000e+00,\n",
    "   7.3883852e-01,  -6.0339369e-01,   1.0000000e+00,\n",
    "   7.0464808e-01,  -2.0420052e-02,   1.0000000e+00,\n",
    "   9.6992666e-01,   6.4137120e-01,  -1.0000000e+00,\n",
    "   4.3543099e-01,   7.4477254e-01,  -1.0000000e+00,\n",
    "  -8.4425822e-01,   7.4235423e-01,   1.0000000e+00,\n",
    "   5.9142471e-01,  -5.4602118e-01,   1.0000000e+00,\n",
    "  -6.9093124e-02,   3.7659995e-02,  -1.0000000e+00,\n",
    "  -9.5154865e-01,  -7.3305502e-01,  -1.0000000e+00,\n",
    "  -1.2988138e-01,   7.5676096e-01,   1.0000000e+00,\n",
    "  -4.9534647e-01,  -5.6627908e-01,  -1.0000000e+00,\n",
    "  -9.0399413e-01,   5.0922150e-01,   1.0000000e+00,\n",
    "   2.9235128e-01,   1.6089015e-01,  -1.0000000e+00,\n",
    "   6.4798552e-01,  -7.7933769e-01,   1.0000000e+00,\n",
    "   3.7595574e-01,   7.8203087e-02,  -1.0000000e+00,\n",
    "   2.4588993e-01,   4.5146739e-03,  -1.0000000e+00,\n",
    "  -4.5719155e-01,   4.2390461e-01,   1.0000000e+00,\n",
    "  -4.4127876e-01,   7.0571892e-01,   1.0000000e+00,\n",
    "   5.0744669e-01,   7.5872586e-01,  -1.0000000e+00,\n",
    "  -1.3258381e-01,  -5.8178837e-01,  -1.0000000e+00,\n",
    "  -4.4749067e-01,   1.9576364e-01,   1.0000000e+00,\n",
    "   8.1658199e-01,  -4.5449182e-01,   1.0000000e+00,\n",
    "  -9.4422408e-01,   8.8273421e-01,   1.0000000e+00,\n",
    "   4.6265533e-01,   3.5583605e-01,  -1.0000000e+00,\n",
    "   8.8311642e-01,  -1.9930013e-01,   1.0000000e+00,\n",
    "   1.0016050e+00,   5.2575476e-01,  -1.0000000e+00,\n",
    "   6.0370095e-01,  -5.4553701e-01,   1.0000000e+00,\n",
    "  -1.4858757e-01,  -2.1308372e-01,  -1.0000000e+00,\n",
    "   1.1652163e-02,   8.8923931e-01,   1.0000000e+00 ]\n",
    ").reshape((35, 3))\n",
    "\n",
    "out_dta = np.array(\n",
    "[ -1.0600562e-01,  -8.1467034e-02,  -1.0000000e+00,\n",
    "   1.7792951e-01,  -3.4595141e-01,  -1.0000000e+00,\n",
    "   1.0216153e-01,   7.1825825e-01,   1.0000000e+00,\n",
    "   6.9407831e-01,   6.2339743e-01,  -1.0000000e+00,\n",
    "   2.3541068e-02,   7.2743221e-01,   1.0000000e+00,\n",
    "  -3.1972776e-01,  -8.3411411e-01,  -1.0000000e+00,\n",
    "  -1.8674372e-01,   5.3887798e-01,   1.0000000e+00,\n",
    "  -6.3696719e-01,   1.5268485e-01,   1.0000000e+00,\n",
    "  -4.7446260e-01,   8.5434436e-01,   1.0000000e+00,\n",
    "  -3.5627652e-02,  -2.7158819e-01,  -1.0000000e+00,\n",
    "  -1.4860269e-01,   1.6176177e-01,  -1.0000000e+00,\n",
    "  -1.8065154e-01,  -1.2873906e-01,  -1.0000000e+00,\n",
    "  -6.0241113e-01,   9.2550746e-01,   1.0000000e+00,\n",
    "   6.9808093e-01,   7.9474170e-01,  -1.0000000e+00,\n",
    "   8.8150888e-01,  -2.0124822e-01,   1.0000000e+00,\n",
    "  -9.2384936e-01,   3.8662506e-01,   1.0000000e+00,\n",
    "  -7.6571338e-01,  -1.1281293e-02,   1.0000000e+00,\n",
    "   1.3559198e-01,   3.1705057e-02,  -1.0000000e+00,\n",
    "  -1.5515148e-01,  -3.3141997e-01,  -1.0000000e+00,\n",
    "   4.8517476e-01,   2.9903104e-01,  -1.0000000e+00,\n",
    "  -6.0290010e-01,   3.3323420e-01,   1.0000000e+00,\n",
    "  -5.7285816e-01,   8.2835226e-01,   1.0000000e+00,\n",
    "  -6.3539998e-01,  -4.7456571e-01,  -1.0000000e+00,\n",
    "   9.0931654e-01,  -7.8488922e-01,   1.0000000e+00,\n",
    "   2.5210516e-01,  -8.9393713e-01,   1.0000000e+00,\n",
    "  -5.1763422e-01,   9.6044364e-01,   1.0000000e+00,\n",
    "  -3.8587212e-01,  -3.1786995e-01,  -1.0000000e+00,\n",
    "   8.2316659e-01,  -1.2779654e-01,   1.0000000e+00,\n",
    "   8.2248638e-01,  -8.7684295e-01,   1.0000000e+00,\n",
    "  -5.0366208e-01,   9.8027386e-01,   1.0000000e+00,\n",
    "   5.3387369e-01,   8.2123374e-01,  -1.0000000e+00,\n",
    "  -8.9497008e-01,  -2.4011485e-01,   1.0000000e+00,\n",
    "   3.4287096e-01,   4.7497683e-01,  -1.0000000e+00,\n",
    "   7.0928861e-01,   5.6220681e-01,  -1.0000000e+00,\n",
    "  -1.0004317e+00,   6.0457567e-02,   1.0000000e+00,\n",
    "   5.2428439e-01,   7.3519522e-01,  -1.0000000e+00,\n",
    "  -5.6033040e-01,   7.5583819e-01,   1.0000000e+00,\n",
    "   6.9752202e-01,  -6.7198955e-01,   1.0000000e+00,\n",
    "   4.9042314e-01,   7.8508662e-01,  -1.0000000e+00,\n",
    "  -3.2677396e-01,   3.4337193e-01,   1.0000000e+00,\n",
    "  -2.9342093e-03,  -4.1518173e-01,  -1.0000000e+00,\n",
    "  -6.3123891e-01,   3.5263395e-01,   1.0000000e+00,\n",
    "   9.1388134e-01,   5.9305320e-01,  -1.0000000e+00,\n",
    "   2.1828280e-01,   3.9683537e-02,  -1.0000000e+00,\n",
    "  -6.1618517e-01,  -8.8657924e-01,  -1.0000000e+00,\n",
    "  -5.2852914e-01,   2.8690210e-02,   1.0000000e+00,\n",
    "  -4.0652261e-01,   1.0451480e+00,   1.0000000e+00,\n",
    "  -2.2979506e-01,   7.1425065e-02,  -1.0000000e+00,\n",
    "  -5.0212113e-01,   8.3373782e-01,   1.0000000e+00,\n",
    "  -5.0808021e-01,   7.9326965e-01,   1.0000000e+00,\n",
    "  -7.9067809e-01,   1.8780315e-01,   1.0000000e+00,\n",
    "  -3.8251119e-01,   8.2474204e-01,   1.0000000e+00,\n",
    "   8.2232787e-01,   4.0148690e-01,  -1.0000000e+00,\n",
    "   9.8596447e-01,  -3.2916872e-01,   1.0000000e+00,\n",
    "  -1.4046951e-02,  -1.5238711e-01,  -1.0000000e+00,\n",
    "  -5.4165125e-02,   9.1428474e-01,   1.0000000e+00,\n",
    "  -1.0724684e+00,  -7.2028556e-01,  -1.0000000e+00,\n",
    "  -2.4298545e-01,  -1.0426515e+00,   1.0000000e+00,\n",
    "  -3.2448585e-01,  -2.8317976e-01,  -1.0000000e+00,\n",
    "   2.4774928e-01,  -2.5565619e-01,  -1.0000000e+00,\n",
    "  -1.7221056e-01,  -8.4939971e-01,   1.0000000e+00,\n",
    "  -4.1726341e-01,  -3.9327101e-01,  -1.0000000e+00,\n",
    "  -3.4783792e-01,  -5.7380853e-01,  -1.0000000e+00,\n",